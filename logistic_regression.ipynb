{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The sigmoid function\n",
    "$$\\boxed{\\phi(z) = \\frac{1}{1+ e^{-z}}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "import numpy as np\n",
    "def sigmoid(z):\n",
    "    return 1.0/(1.0 + np.exp(-z))\n",
    "z = np.arange(-7, 7, 0.1)\n",
    "phi_z = sigmoid(z)\n",
    "plt.plot(z, phi_z)\n",
    "plt.axvline(0.0, color = 'k')\n",
    "plt.ylim(-0.1, 1.1)\n",
    "plt.xlabel('z')\n",
    "plt.ylabel('$\\phi (z)$')\n",
    "plt.yticks([0.0, 0.5, 1.0])\n",
    "ax = plt.gca()\n",
    "ax.yaxis.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regrsssion\n",
    "Logistic regression is a classification model. Asumme that we are dealing with a binary classification problem  where all data points are in $\\mathbb{R}^d$ and their labels are either $0$ or $1$. Given a weight vector $w \\in \\mathbb{R}^d$, we can define the probability that an arbitrary data point $x$ belong to class 1 by\n",
    "$$\\mathbb{P}(y=1| x,w) = \\phi (w^Tx) = \\frac{1}{1 + e^{-w^Tx}}.$$\n",
    "The probability that $x$ belongs to class $0$ is then \n",
    "$$\\mathbb{P}(y=0| x,w) = 1- \\phi (w^Tx) = \\frac{1}{1 + e^{w^Tx}}.$$\n",
    "The outcome of the logistic regression is then \n",
    "$$\\hat{y} = argmax\\{\\mathbb{P}(y=0| x,w), \\mathbb{P}(y=1| x,w)\\}.$$\n",
    "### Learning the weight of the logistic cost function\n",
    "In order to find the optimal weight $w^*$, we minimize the following cost function\n",
    "$$ \\boxed{J(w) = \\sum_{i=1}^{n}\\left[-y_i\\log(\\phi(z_i))- (1-y_i)\\log(1-\\phi(z_i))\\right].}$$\n",
    "We compute the gradient  and the Hessian of $J$. We have \n",
    "\\begin{align*}\n",
    "\\nabla_{w} J(w)& = -\\sum_{i = 1}^{n}\\left[y_i\\frac{1}{\\phi(z_i)}\\nabla_{w}\\phi(z_i)-(1-y_i)\\frac{1}{1-\\phi(z_i)}\\nabla_w\\phi(z_i)\\right]\\\\\n",
    "& = -\\sum_{i=1}^{n}\\left[y_i\\frac{1}{\\phi(z_i)}-(1-y_i)\\frac{1}{1-\\phi(z_i)}\\right]\\phi'(z_i)x_i\\\\\n",
    "& = -\\sum_{i=1}^{n}\\left[y_i\\frac{1}{\\phi(z_i)}-(1-y_i)\\frac{1}{1-\\phi(z_i)}\\right]\\phi(z_i)(1- \\phi(z_i))x_i\\\\\n",
    "& = -\\sum_{i=1}^n[y_i - \\phi(z_i)]x_i.\n",
    "\\end{align*}\n",
    "Thus formula for the gradient of $J$ is \n",
    "$$\\boxed{\\nabla_w J(w) = \\sum_{i=1}^n[\\phi(z_i)-y_i]x_i = X^T(\\phi-y).}$$\n",
    "where \n",
    "$$\n",
    "X = \\begin{bmatrix}\n",
    "x_1^T\\\\\n",
    "\\vdots\\\\\n",
    "x_n^T\n",
    "\\end{bmatrix},\n",
    "\\hspace{1cm}\n",
    "y = \\begin{bmatrix}\n",
    "y_1\\\\\n",
    "\\vdots\\\\\n",
    "y_n\n",
    "\\end{bmatrix},\n",
    "\\hspace{1cm}\n",
    "\\begin{bmatrix}\n",
    "\\phi(z_1)\\\\\n",
    "\\vdots\\\\\n",
    "\\phi(z_n)\n",
    "\\end{bmatrix}.\n",
    "$$\n",
    "In a similar way, we obtain the following formula for the Hessian of $J$ \n",
    "$$\n",
    "\\boxed{\\nabla^2_{w} J(w) = \\sum_{i=1}^m\\phi(z_i)(1-\\phi(z_i))x_ix_i^T = Xdiag\\{\\phi\\odot(1-\\phi)\\}X^T.}\n",
    "$$\n",
    "By applying the gradient descent  with stepsize $\\eta$, we obtain the following update rule\n",
    "\n",
    "\\begin{align*}\n",
    "w_0 &= \\hat{w}\\\\\n",
    "w_{t+1} &= w_t - \\eta X^T(\\phi-y).\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n",
    "X = iris.data[:, [2,3]]\n",
    "y = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 1, stratify = y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "sc.fit(X_train)\n",
    "X_train_std = sc.transform(X_train)\n",
    "X_test_std = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.33269725, -1.30728421],\n",
       "       [-1.16537974, -1.30728421],\n",
       "       [ 0.84243039,  1.44587881],\n",
       "       [ 1.0655204 ,  1.18367281],\n",
       "       [-1.44424226, -1.30728421],\n",
       "       [ 1.0097479 ,  1.57698181],\n",
       "       [ 1.56747294,  1.18367281],\n",
       "       [-1.44424226, -1.30728421],\n",
       "       [ 1.12129291,  0.5281578 ],\n",
       "       [ 0.45202286,  0.1348488 ],\n",
       "       [-0.27301968, -0.2584602 ],\n",
       "       [ 0.06161534,  0.2659518 ],\n",
       "       [-1.38846976, -1.30728421],\n",
       "       [ 0.50779537,  0.0037458 ],\n",
       "       [ 0.11738784,  0.1348488 ],\n",
       "       [ 0.73088538,  0.92146681],\n",
       "       [-1.05383474, -1.30728421],\n",
       "       [-0.16147468, -0.2584602 ],\n",
       "       [ 0.06161534,  0.0037458 ],\n",
       "       [-1.22115225, -1.30728421],\n",
       "       [ 0.56356787,  0.79036381],\n",
       "       [ 1.73479045,  1.44587881],\n",
       "       [ 0.39625036,  0.3970548 ],\n",
       "       [ 0.39625036,  0.1348488 ],\n",
       "       [ 0.00584283, -0.1273572 ],\n",
       "       [ 1.0097479 ,  1.57698181],\n",
       "       [ 0.50779537,  0.2659518 ],\n",
       "       [ 1.0097479 ,  0.2659518 ],\n",
       "       [ 1.12129291,  1.31477581],\n",
       "       [ 0.73088538,  1.57698181],\n",
       "       [ 0.17316034,  0.1348488 ],\n",
       "       [-1.27692475, -1.04507821],\n",
       "       [ 1.62324544,  1.31477581],\n",
       "       [ 0.67511288,  0.92146681],\n",
       "       [ 0.56356787,  0.79036381],\n",
       "       [ 1.0097479 ,  1.18367281],\n",
       "       [ 0.22893285,  0.3970548 ],\n",
       "       [ 1.62324544,  1.05256981],\n",
       "       [ 0.9539754 ,  0.79036381],\n",
       "       [-1.22115225, -1.30728421],\n",
       "       [ 0.61934037,  0.79036381],\n",
       "       [-1.33269725, -1.30728421],\n",
       "       [ 0.73088538,  0.3970548 ],\n",
       "       [-1.05383474, -1.04507821],\n",
       "       [-1.55578727, -1.30728421],\n",
       "       [ 0.61934037,  0.3970548 ],\n",
       "       [-1.27692475, -1.30728421],\n",
       "       [-1.50001477, -1.43838721],\n",
       "       [ 0.9539754 ,  0.79036381],\n",
       "       [ 0.50779537,  0.3970548 ],\n",
       "       [-1.16537974, -1.17618121],\n",
       "       [-0.16147468, -0.2584602 ],\n",
       "       [ 0.17316034, -0.2584602 ],\n",
       "       [-1.27692475, -1.30728421],\n",
       "       [-1.27692475, -1.30728421],\n",
       "       [-1.27692475, -1.30728421],\n",
       "       [-0.04992967, -0.2584602 ],\n",
       "       [ 1.28861042,  1.70808482],\n",
       "       [-1.38846976, -1.17618121],\n",
       "       [ 0.61934037,  0.3970548 ],\n",
       "       [-1.38846976, -1.30728421],\n",
       "       [ 0.39625036,  0.5281578 ],\n",
       "       [ 1.23283791,  0.79036381],\n",
       "       [-1.22115225, -1.30728421],\n",
       "       [-1.33269725, -1.30728421],\n",
       "       [ 0.34047786,  0.0037458 ],\n",
       "       [ 0.73088538,  0.92146681],\n",
       "       [-0.10570217,  0.1348488 ],\n",
       "       [ 0.17316034,  0.1348488 ],\n",
       "       [ 0.56356787,  0.79036381],\n",
       "       [ 1.28861042,  1.44587881],\n",
       "       [ 0.39625036,  0.3970548 ],\n",
       "       [ 0.39625036,  0.3970548 ],\n",
       "       [ 0.89820289,  1.18367281],\n",
       "       [ 1.0097479 ,  1.31477581],\n",
       "       [ 0.45202286,  0.2659518 ],\n",
       "       [-1.27692475, -1.04507821],\n",
       "       [-1.16537974, -0.91397521],\n",
       "       [ 1.28861042,  0.92146681],\n",
       "       [ 0.73088538,  0.5281578 ],\n",
       "       [ 1.45592793,  1.05256981],\n",
       "       [ 0.67511288,  0.3970548 ],\n",
       "       [-1.27692475, -1.30728421],\n",
       "       [ 0.22893285,  0.1348488 ],\n",
       "       [ 0.73088538,  1.05256981],\n",
       "       [ 1.23283791,  1.70808482],\n",
       "       [-1.27692475, -1.17618121],\n",
       "       [-1.33269725, -1.30728421],\n",
       "       [ 1.0097479 ,  0.79036381],\n",
       "       [-1.33269725, -1.17618121],\n",
       "       [ 1.17706541,  1.44587881],\n",
       "       [ 0.06161534, -0.1273572 ],\n",
       "       [ 0.22893285,  0.0037458 ],\n",
       "       [ 0.28470535,  0.1348488 ],\n",
       "       [-1.27692475, -1.43838721],\n",
       "       [ 0.78665788,  1.44587881],\n",
       "       [ 0.34047786,  0.1348488 ],\n",
       "       [-1.38846976, -1.30728421],\n",
       "       [-1.33269725, -1.17618121],\n",
       "       [ 0.67511288,  0.65926081],\n",
       "       [-1.38846976, -1.17618121],\n",
       "       [ 0.11738784,  0.0037458 ],\n",
       "       [-1.22115225, -1.04507821],\n",
       "       [-1.27692475, -1.30728421],\n",
       "       [-1.38846976, -1.30728421]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegressionGD(object):\n",
    "    \"\"\"\"\"\n",
    "    Short explaination of the class\n",
    "    \"\"\"\"\"\n",
    "    \n",
    "    def __init__(self, eta = 0.05, n_iter = 100, random_state = 1):\n",
    "        self.eta = eta\n",
    "        self.n_iter = n_iter\n",
    "        self.random_state = random_state\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        rgen = np.random.RandomState(self.random_state)\n",
    "        self.w_ = rgen.normal(loc = 0.0, scale = 0.01, size = 1 +X.shape[1])\n",
    "        self.cost_ = []\n",
    "        \n",
    "        for i in range(self.n_iter):\n",
    "            net_input = self.net_input(X)\n",
    "            output = self.activation(net_input)\n",
    "            errors = y - output\n",
    "            self.w_[1:] += self.eta * X.T.dot(errors)\n",
    "            self.w_[0] += self.eta * errors.sum()\n",
    "            cost = -y.dot(np.log(output)) -(1-y).dot(np.log(1-output))\n",
    "            self.cost_.append(cost)\n",
    "        return self\n",
    "    \n",
    "    def net_input(self, X):\n",
    "        return np.dot(X,self.w_[1:]) + self.w_[0]\n",
    "    \n",
    "    def activation(self, z):\n",
    "        return 1./(1. + np.exp(-np.clip(z, -250, 250)))\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return np.where(self.net_input(X) >= 0.0, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_01_subset = X_train[(y_train == 0) | (y_train == 1)]\n",
    "y_train_01_subset = y_train[(y_train == 0) | (y_train == 1)]\n",
    "lrgd = LogisticRegressionGD(eta=0.05, n_iter=1000, random_state=1)\n",
    "lrgd.fit(X_train_01_subset, y_train_01_subset)\n",
    "y_pre = lrgd.predict(X_train_01_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_true = y_train_01_subset, y_pred = y_pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9523809523809523"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression(C=100.0, random_state = 1, solver= 'lbfgs', multi_class = 'auto')\n",
    "lr.fit(X_train_std, y_train)\n",
    "y_hat = lr.predict(X_train_std)\n",
    "accuracy_score(y_true = y_train, y_pred = y_hat)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
